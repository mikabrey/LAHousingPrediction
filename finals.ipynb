{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # library for data analysis\n",
    "from pandas import DataFrame\n",
    "import requests # library to handle requests\n",
    "from bs4 import BeautifulSoup # library to parse HTML documents\n",
    "import numpy as np # library to handle data in a vectorized manner\n",
    "import json # library to handle JSON files\n",
    "#!conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
    "from geopy.geocoders import Nominatim # convert an address into latitude and longitude value\n",
    "from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n",
    "# Matplotlib and associated plotting modules\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "# import k-means from clustering stage\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#!conda install -c conda-forge folium=0.5.0 --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
    "import folium # map rendering library\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, confusion_matrix,r2_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn import datasets, ensemble\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import NullFormatter\n",
    "from sklearn import metrics\n",
    "import matplotlib.ticker as ticker\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import mpl_toolkits\n",
    "\n",
    "\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018 Tax data to evaluate location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://data.ftb.ca.gov/California-Personal-Income-Tax/Personal-Income-Tax-Statistics-By-Zip-Code/mriu-wsxf\n",
    "# 2018 is max available\n",
    "url='https://data.ftb.ca.gov/resource/mriu-wsxf.json?$where=county=\"Los Angeles\" and taxable_year = 2018 limit 1000000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsi = requests.get(url).json()\n",
    "income = pd.DataFrame.from_records(data=resultsi)\n",
    "# filter columns df.sort(['A', 'B'], ascending=[1, 0])df.sort_values(by='col1', ascending=False)df.sort_values(['Total Due'], inplace=True)\n",
    "income.sort_values(['zip_code'],ascending=True,inplace=True)\n",
    "print(income[income.isna().any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correct Data type\n",
    "\n",
    "income['zip_code']=income['zip_code'].astype(str)\n",
    "income['ca_agi'] = income['ca_agi'].astype(int)\n",
    "income['returns'] = income['returns'].astype(int)\n",
    "income['total_tax_liability']=income['total_tax_liability'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average Home Price by zip city of LA\n",
    "#wikiurl=\"http://www.laalmanac.com/economy/ec37b.php\"\n",
    "wikiurl =\"http://www.laalmanac.com/economy/ec37c.php\" \n",
    "table=\"\"\n",
    "response2=requests.get(wikiurl)\n",
    "print(response2.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response2.text, 'html.parser')\n",
    "indiatable=soup.find('table')\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data\n",
    "df.columns = [\"zip_code\",\"City/community\",\"2020\",\"2019\",\"2018\",\"2017\",\"2016\",\"2015\"]\n",
    "#df.set_index(\"zip code\",inplace= True)\n",
    "print(df[df.isna().any(axis=1)])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column=['year','zip_code','city/community','condo price']\n",
    "cond=[]\n",
    "cond=pd.DataFrame(columns=column)\n",
    "ran=df.shape[0]\n",
    "i=0\n",
    "for i in range(ran) :\n",
    "    post = df.iloc[i][0]   \n",
    "    city = df.iloc[i][1]\n",
    "    price1 = df.iloc[i][2]\n",
    "    price2 = df.iloc[i][3]\n",
    "    price3 = df.iloc[i][4]\n",
    "    price4 = df.iloc[i][5]\n",
    "    price5 = df.iloc[i][6]\n",
    "    price6 = df.iloc[i][7]\n",
    "    i=i+1    \n",
    "\n",
    "    cond=cond.append({'year':\"2020\",'zip_code':post,'city/community':city,'condo price':price1},ignore_index=True)\n",
    "    cond=cond.append({'year':'2019','zip_code':post,'city/community':city,'condo price':price2},ignore_index=True)\n",
    "    cond=cond.append({'year':'2018','zip_code':post,'city/community':city,'condo price':price3},ignore_index=True)\n",
    "    cond=cond.append({'year':'2017','zip_code':post,'city/community':city,'condo price':price4},ignore_index=True)\n",
    "    cond=cond.append({'year':'2016','zip_code':post,'city/community':city,'condo price':price5},ignore_index=True)\n",
    "    cond=cond.append({'year':'2015','zip_code':post,'city/community':city,'condo price':price6},ignore_index=True)\n",
    "cond.dropna(subset=['condo price'],inplace=True)\n",
    "cond.shape\n",
    "cond['zip_code']=cond['zip_code'].astype(str)\n",
    "cond['condo price'] = cond['condo price'].replace({'\\$': '', ',': ''}, regex=True).astype(int)    \n",
    "cond.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# marge income and condo price table˘ note only have one income year˘˘˘˘˘\n",
    "\n",
    "la_df = pd.merge(cond, income, on='zip_code',how='left')\n",
    "la_df.drop(['taxable_year','state','county','countylatitude','countylongitude','geo_county','geo_city'],axis = 'columns', inplace=True)\n",
    "#la_df['price'] = la_df['price'].replace({'\\$': '', ',': ''}, regex=True).astype(int)\n",
    "la_df['income'] = la_df['ca_agi']/la_df['returns']\n",
    "la_df['liability']=la_df['total_tax_liability']/la_df['returns']\n",
    "#merge_df['total_tax_liabillity'] = merge_df['total_tax_liabillity'].astype(float)\n",
    "la_df.drop(['ca_agi','total_tax_liability'],axis = 'columns', inplace=True)\n",
    "la_df.rename({'zip_code':'zip code'},axis ='columns',inplace =True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(la_df[la_df.isna().any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://fred.stlouisfed.org/series/CASTHPI\n",
    "#Unenployment LA County 1990 ~\n",
    "unemp = pd.read_excel('CALOSA7URN.xls') \n",
    "print (unemp.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlf2020='https://data.lacity.org/resource/kde4-k7yx.json?$limit=100000&$offset=0'\n",
    "urlf2015='https://data.lacity.org/resource/f37w-ye7d.json?$limit=100000&$offset=0'\n",
    "urlf2016='https://data.lacity.org/resource/4sbs-dcfn.json?$limit=100000&$offset=0'\n",
    "urlf2017='https://data.lacity.org/resource/r53k-qp8f.json?$limit=100000&$offset=0'\n",
    "urlf2018='https://data.lacity.org/resource/e6cg-sqdy.json?$limit=100000&$offset=0'\n",
    "urlf2019='https://data.lacity.org/resource/rsxb-x48z.json?$limit=100000&$offset=0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getfore(url,wyear):\n",
    "    resultsf = requests.get(url).json()\n",
    "    foreclose = pd.DataFrame.from_records(data=resultsf)\n",
    "    \n",
    "    \n",
    "    if wyear == '2015':\n",
    "        foreclose['regdate'] = foreclose['registereddate'].astype('datetime64[ns]')\n",
    "        foreclose['yearmonth']=foreclose['regdate'].dt.strftime('%Y-%m')\n",
    "        foreclose['yearmonth1st'] = foreclose['yearmonth'] + '-01'\n",
    "        select_col=foreclose[[\"yearmonth1st\",\"propertyzip\",\"apn\"]]\n",
    "        forecls=select_col.copy()\n",
    "        forecls.sort_values(['yearmonth1st','propertyzip'],ascending=True,inplace=True)\n",
    "        forecls.fillna('2015-0w-01',inplace=True)\n",
    "        \n",
    "    if wyear == '2016':\n",
    "        foreclose['regdate'] = foreclose['registereddate'].astype('datetime64[ns]')\n",
    "        foreclose['yearmonth']=foreclose['regdate'].dt.strftime('%Y-%m')\n",
    "        foreclose['yearmonth1st'] = foreclose['yearmonth'] + '-01'\n",
    "        select_col=foreclose[[\"yearmonth1st\",\"propertyzip\",\"apn\"]]\n",
    "        forecls=select_col.copy()\n",
    "        forecls.sort_values(['yearmonth1st','propertyzip'],ascending=True,inplace=True)        \n",
    "        forecls.fillna('2016-02-01',inplace=True)\n",
    "    if wyear == '2017':\n",
    "        foreclose['regdate'] = foreclose['registered_date'].astype('datetime64[ns]')\n",
    "        foreclose['yearmonth']=foreclose['regdate'].dt.strftime('%Y-%m')\n",
    "        foreclose['yearmonth1st'] = foreclose['yearmonth'] + '-01'\n",
    "        select_col=foreclose[[\"yearmonth1st\",\"propertyzip\",\"apn\"]]\n",
    "        forecls=select_col.copy()\n",
    "        forecls.sort_values(['yearmonth1st','propertyzip'],ascending=True,inplace=True)        \n",
    "        forecls.fillna('2017-02-01',inplace=True)\n",
    "    if wyear == '2018':\n",
    "        foreclose['regdate'] = foreclose['registereddate'].astype('datetime64[ns]')\n",
    "        foreclose['yearmonth']=foreclose['regdate'].dt.strftime('%Y-%m')\n",
    "        foreclose['yearmonth1st'] = foreclose['yearmonth'] + '-01'\n",
    "        select_col=foreclose[[\"yearmonth1st\",\"propertyzip\",\"apn\"]]\n",
    "        forecls=select_col.copy()\n",
    "        forecls.sort_values(['yearmonth1st','propertyzip'],ascending=True,inplace=True)\n",
    "        forecls.fillna('2018-02-01',inplace=True)\n",
    "    if wyear == '2019':\n",
    "        foreclose['regdate'] = foreclose['registered_date'].astype('datetime64[ns]')\n",
    "        foreclose['yearmonth']=foreclose['regdate'].dt.strftime('%Y-%m')\n",
    "        foreclose['yearmonth1st'] = foreclose['yearmonth'] + '-01'\n",
    "        select_col=foreclose[[\"yearmonth1st\",\"propertyzip\",\"apn\"]]\n",
    "        forecls=select_col.copy()\n",
    "        forecls.sort_values(['yearmonth1st','propertyzip'],ascending=True,inplace=True)\n",
    " \n",
    "        forecls.fillna('2019-02-01',inplace=True)\n",
    "    if wyear == '2020':\n",
    "        foreclose['regdate'] = foreclose['registered_date'].astype('datetime64[ns]')\n",
    "        foreclose['yearmonth']=foreclose['regdate'].dt.strftime('%Y-%m')\n",
    "        foreclose['yearmonth1st'] = foreclose['yearmonth'] + '-01'\n",
    "        select_col=foreclose[[\"yearmonth1st\",\"propertyzip\",\"apn\"]]\n",
    "        forecls=select_col.copy()\n",
    "        forecls.sort_values(['yearmonth1st','propertyzip'],ascending=True,inplace=True)\n",
    "        forecls.fillna('2020-02-01',inplace=True)\n",
    "    return (forecls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foreclosea=[]\n",
    "forecls=[]\n",
    "forecls=getfore(urlf2015,'2015')\n",
    "\n",
    "foreclosea = forecls.copy()\n",
    "forecls=getfore(urlf2016,'2016')\n",
    "foreclosea = foreclosea.append(forecls)\n",
    "\n",
    "forecls=getfore(urlf2017,'2017')\n",
    "foreclosea = foreclosea.append(forecls)\n",
    "\n",
    "forecls=getfore(urlf2018,'2018')\n",
    "foreclosea = foreclosea.append(forecls)\n",
    "\n",
    "forecls=getfore(urlf2019,'2019')\n",
    "foreclosea = foreclosea.append(forecls)\n",
    "\n",
    "forecls=getfore(urlf2020,'2020')\n",
    "foreclosea = foreclosea.append(forecls)\n",
    "\n",
    "#delete na\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foreclosea.dropna(subset=['yearmonth1st'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foreclosea.rename({'propertyzip':'zip code','apn':'foreclosure'},axis='columns',inplace=True)\n",
    "foreclosea.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foreclose_grp=foreclosea.groupby(['yearmonth1st','zip code']).count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foreclose_grp['yearmonth1st']=foreclose_grp['yearmonth1st'].astype(str)\n",
    "foreclose_grp['zip code']=foreclose_grp['zip code'].astype(str)\n",
    "foreclose_grp.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete zip = 0 zip = 999, zip = 2018-01-01\n",
    "indexnm=foreclose_grp[foreclose_grp['zip code']=='0'].index\n",
    "foreclose_grp.drop(indexnm,inplace = True)\n",
    "indexnm=foreclose_grp[foreclose_grp['zip code']=='99999'].index\n",
    "foreclose_grp.drop(indexnm,inplace = True)\n",
    "\n",
    "indexnm=foreclose_grp[foreclose_grp['zip code']=='2018-01-01'].index\n",
    "foreclose_grp.drop(indexnm,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foreclose_grp.groupby(['zip code']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create unemployment for zip 90016,90018,90034\n",
    "#Unenployment LA County 1990 ~\n",
    "#unemp = pd.read_excel('CALOSA7URN.xls') \n",
    "#filter \n",
    "unemp1=unemp.loc[unemp['observation_date'] > '2014-12-31']\n",
    "\n",
    "unemp2020 = unemp1.loc[unemp1['observation_date']==unemp1['observation_date'].max()]\n",
    "rate1 = unemp2020.iloc[0][1]\n",
    "print(rate1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unemployment by zip\n",
    "\n",
    "La_Unemployment = 'LAUnemployment.xls'\n",
    "une = pd.read_excel(La_Unemployment)\n",
    "une['zip code']=une['zip_code'].astype(str)\n",
    "une['unemployment']=une['unemployment'].replace({'\\%':'',',':''},regex=True).astype(float)\n",
    "une.drop(labels=['lat','city','rank','zip_code','no','labor','lon'],axis=1,inplace=True)\n",
    "une['per']=une['unemployment']/rate1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column=['zip code','unemployment_rate','yearmonth1st']\n",
    "unemployment=[]\n",
    "unemployment=pd.DataFrame(columns=column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#marge year date and zip unemployment and create new data frame\n",
    "rang=une.shape[0]\n",
    "rang1=unemp1.shape[0]\n",
    "i=0\n",
    "j = 0\n",
    "for j in range(rang1):\n",
    "\n",
    "    for i in range(rang) :\n",
    "        post = une.iloc[i][1]   \n",
    "        rate =  une.iloc[i][2] * unemp1.iloc[j][1]\n",
    "        i=i+1    \n",
    "        yearmonth = unemp1.iloc[j][0]\n",
    "        unemployment=unemployment.append({'zip code':post,'unemployment_rate':rate,'yearmonth1st':yearmonth},ignore_index=True)\n",
    "    j=j +1\n",
    "\n",
    "unemployment.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment['yearmonth1st']=unemployment['yearmonth1st'].astype(str)\n",
    "unemployment['zip code']=unemployment['zip code'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#marge foreclosure and unemployment\n",
    "mydata = pd.merge(unemployment, foreclose_grp , on=['zip code' , 'yearmonth1st'],how='left')\n",
    "mydata['foreclosure'].fillna(0,inplace = True)\n",
    "mydata['year']=mydata['yearmonth1st'].str.slice(start=0,stop=4)\n",
    "mydata=mydata.loc[mydata['yearmonth1st'] < '2020-01-01' ]\n",
    "mydata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata1=pd.merge(mydata,la_df, on =['zip code','year'],how = 'left')\n",
    "mydata1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata1=pd.merge(mydata,la_df, on =['zip code','year'],how = 'left')\n",
    "column=['zip code','unemployment_rate','yearmonth','foreclosure','income','condprice','lat','lon']\n",
    "mydatas=[]\n",
    "mydatas=pd.DataFrame(columns=column)\n",
    "#print(mydata1[mydata1.isna().any(axis=1)])\n",
    "# remove nan\n",
    "mydata1=mydata1.dropna()\n",
    "rang=mydata1.shape[0]\n",
    "\n",
    "i = 0\n",
    "for i in range(rang):\n",
    "    post = mydata1.iloc[i][0]   \n",
    "    rate=  mydata1.iloc[i][1]\n",
    "    yddate = mydata1.iloc[i][2]\n",
    "    foreclosure = mydata1.iloc[i][3]\n",
    "    loc=mydata1.iloc[i][9]\n",
    "    geo=pd.DataFrame.from_records(loc)\n",
    "    lon = (geo.coordinates[0])\n",
    "    lat= (geo.coordinates[1])\n",
    "    cond = mydata1.iloc[i][6]\n",
    "    income=mydata1.iloc[i][10]\n",
    "    i=i+1    \n",
    "    mydatas=mydatas.append({'zip code':post,'unemployment_rate':rate,'yearmonth':yddate,'foreclosure':foreclosure,'lon':lon,'lat':lat,'condprice':cond,'income':income},ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rowsx = mydatas.groupby('zip code').agg({'foreclosure':'sum'}).reset_index()\n",
    "\n",
    "rows1=rowsx.loc[rowsx['foreclosure'] <= 0.0 ]\n",
    "print (rows1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydatas.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydatas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_grouped=mydatas.groupby('zip code').agg({'unemployment_rate':'mean','foreclosure':'sum','lat':'mean','lon':'mean'}).reset_index()\n",
    "\n",
    "la_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude= la_grouped.iloc[0][3]\n",
    "longitude = la_grouped.iloc[0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydatas['zip code']=mydatas['zip code'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show Foreclosure and unemployment on the map\n",
    "from folium.plugins import MarkerCluster\n",
    "#geo = pd.DataFrame.from_records(mydata1.loc[3, 'geo_zipcode'])\n",
    "\n",
    "lamap=folium.Map(location = [latitude,longitude],\n",
    "default_zoom_start=8)\n",
    "marker_cluster=MarkerCluster().add_to(lamap)\n",
    "# add a maker for foreclosur\n",
    "for lat,lon,poi, forecl in zip(la_grouped['lat'],la_grouped['lon'], la_grouped['zip code'],la_grouped['foreclosure']):\n",
    "    label =  'foreclosure:' + '{}'.format(forecl) \n",
    "#    label = folium.Popup(label, parse_html=True)\n",
    "    folium.Marker(\n",
    "        [lat, lon],\n",
    "        popup=label).add_to(marker_cluster)\n",
    "Zipc='ZIPCodes.geojson'\n",
    "folium.Choropleth(\n",
    "    geo_data=Zipc,\n",
    "    data=la_grouped,\n",
    "    columns=['zip code','unemployment_rate'],\n",
    "    key_on='feature.properties.zipcode',\n",
    "    fill_opacity=0.3,\n",
    "    line_opacity=0.2,\n",
    "    line_weight=2,\n",
    "    legend_name='foreclosure',\n",
    "    fill_color='RdYlGn'\n",
    ").add_to(lamap)\n",
    "folium.LayerControl().add_to(lamap)\n",
    "\n",
    "lamap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and show pairplot\n",
    "z=mydatas[['unemployment_rate','condprice','zip code','income','foreclosure']]\n",
    "sns.pairplot(z, size=2.5)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mydatas.drop(['zip code'],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z=mydatas[['unemployment_rate','condprice','income','foreclosure']]\n",
    "#sns.pairplot(z, size=2.5)\n",
    "#plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple regression zip vs price\n",
    "msk=np.random.rand(len(mydatas))< 0.8\n",
    "train=mydatas[msk]\n",
    "test=mydatas[~msk]\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "fig.add_subplot(2,1,1)\n",
    "plt.scatter(train['yearmonth'],train.unemployment_rate,color='blue')\n",
    "plt.xlabel(\"date\")\n",
    "plt.ylabel(\"unemployment_rate\")\n",
    "plt.show\n",
    "#simple regression zip vs price\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "fig.add_subplot(2,1,1)\n",
    "plt.scatter(train['yearmonth'],train.foreclosure,color='red')\n",
    "plt.xlabel(\"date\")\n",
    "plt.ylabel(\"foreclosure\")\n",
    "plt.show\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#unemployment is very high on bith 20150101 and 20260101lead to overfitting. Therefore I remove those two data\n",
    "mudatas=mydatas.drop(mydatas[mydatas['yearmonth'] =='2015-01-01'].index,inplace=True)\n",
    "mudatas=mydatas.drop(mydatas[mydatas['yearmonth'] =='2016-01-01'].index,inplace=True)\n",
    "mudatas=mydatas.drop(mydatas[mydatas['yearmonth'] =='2017-01-01'].index,inplace=True)\n",
    "mudatas=mydatas.drop(mydatas[mydatas['yearmonth'] =='2018-01-01'].index,inplace=True)\n",
    "mudatas=mydatas.drop(mydatas[mydatas['yearmonth'] =='2019-01-01'].index,inplace=True)\n",
    "#mydatas=mydatas.drop(['yearmonth'],axis =1,inplace=True)\n",
    "mydatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%matplotlib inline\n",
    "#bins = np.linspace(mydatas.unemployment_rate.min(),mydatas.unemployment_rate.max(),10)\n",
    "\n",
    "\n",
    "#g = sns.FacetGrid(mydatas, col='income',hue='foreclosure',palette=\"Set1\",col_wrap = 2)\n",
    "#g.map(plt.hist,'unemployment_rate',bins=bins,ec =\"k\")\n",
    "#g.axes[-1].legend()\n",
    "#plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = 'Los Angeles, CA'\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"LA_explorer\")\n",
    "location = geolocator.geocode(address)\n",
    "latitude = location.latitude\n",
    "longitude = location.longitude\n",
    "print('The geograpical coordinate of LA are {}, {}.'.format(latitude, longitude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Model uding multiple regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID = 'ZH5B0D1UA2ALS2BPY4YFHLDAJTL02J10FTS2STPNUDO5OINP' # your Foursquare ID\n",
    "CLIENT_SECRET = 'EEKM3BVHOQQ2VGK1GYNNFOOYFRYIN0MCXDGOLE0UAC5CEPFE' # your Foursquare Secret\n",
    "VERSION = '20201201' # Foursquare API version\n",
    "LIMIT = 50 # A default Foursquare API limit value\n",
    "\n",
    "print('Your credentails:')\n",
    "print('CLIENT_ID: ' + CLIENT_ID)\n",
    "print('CLIENT_SECRET:' + CLIENT_SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that extracts the category of the venue\n",
    "def get_category_type(row):\n",
    "    try:\n",
    "        categories_list = row['categories']\n",
    "    except:\n",
    "        categories_list = row['venue.categories']\n",
    "        \n",
    "    if len(categories_list) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return categories_list[0]['name']\n",
    "venues_list=[]    \n",
    "venues2_list=[]    \n",
    "venues2_list=[]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = mydatas.groupby(['zip code','lat','lon']).count().reset_index()\n",
    "\n",
    "rows2 = rows\n",
    "rows2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4d4b7105d754a06377d81259 out door 4bf58dd8d48988d12f941735 library 4bf58dd8d48988d1ca941735 pizza\n",
    "#,4bf58dd8d48988d1e0931735,4bf58dd8d48988d1dc931735,4d4b7105d754a06377d81259,4bf58dd8d48988d12f941735,4bf58dd8d48988d1ca941735'\n",
    "\n",
    "#def getNearbyVenues(names, latitudes, longitudes, radius=500):\n",
    "\n",
    "Cat ='4bf58dd8d48988d1ca941735,4bf58dd8d48988d16d941735,4bf58dd8d48988d1e0931735,4bf58dd8d48988d1dc931735'\n",
    "name=la_df['zip code']\n",
    "radius = 500\n",
    "for name, loc in zip(la_df['zip code'], la_df['geo_zipcode'] ):\n",
    "    \n",
    "        #print(name)\n",
    "    geo = pd.DataFrame.from_records(data=loc)\n",
    "    lng = (geo.coordinates[0])\n",
    "    lat= (geo.coordinates[1])        \n",
    "        # create the API request URL\n",
    "\n",
    "    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&categoryId={}'.format(\n",
    "        CLIENT_ID, \n",
    "        CLIENT_SECRET, \n",
    "        VERSION, \n",
    "        lat, \n",
    "        lng, \n",
    "        radius, \n",
    "        LIMIT,\n",
    "        Cat)\n",
    "    url\n",
    "        # make the GET request\n",
    "    results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
    "        \n",
    "        # return only relevant information for each nearby venue\n",
    "        # return only relevant information for each nearby venue\n",
    "    venues_list.append([(\n",
    "        name, \n",
    "        lat, \n",
    "        lng, \n",
    "        v['venue']['name'], \n",
    "        v['venue']['location']['lat'], \n",
    "        v['venue']['location']['lng'],  \n",
    "        v['venue']['categories'][0]['name']) for v in results])\n",
    "\n",
    "nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
    "nearby_venues.columns = ['Neighborhood', \n",
    "                'Neighborhood Latitude', \n",
    "                'Neighborhood Longitude', \n",
    "                'Venue', \n",
    "                'Venue Latitude', \n",
    "                'Venue Longitude', \n",
    "                'Venue Category']\n",
    "    #return(nearby_venues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4bf58dd8d48988d16d941735 cafa  4bf58dd8d48988d1e0931735 coffee shop 4bf58dd8d48988d1dc931735 tea room\n",
    "#4d4b7105d754a06377d81259 out door 4bf58dd8d48988d12f941735 library 4bf58dd8d48988d1ca941735 pizza\n",
    "                            \n",
    "venues1_list=[]    \n",
    "Cat ='4d4b7105d754a06377d81259,4bf58dd8d48988d1ca941735'\n",
    "name=rows2\n",
    "radius = 500\n",
    "for name, lat,lng in zip(rows2['zip code'], rows2['lat'],rows2['lon'] ):\n",
    "    \n",
    "        #print(name)\n",
    "  #  geo = pd.DataFrame.from_records(data=loc)\n",
    "  #  lng = (geo.coordinates[0])\n",
    "  #  lat= (geo.coordinates[1])        \n",
    "        # create the API request URL\n",
    "\n",
    "    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&categoryId={}'.format(\n",
    "        CLIENT_ID, \n",
    "        CLIENT_SECRET, \n",
    "        VERSION, \n",
    "        lat, \n",
    "        lng, \n",
    "        radius, \n",
    "        LIMIT,\n",
    "        Cat)\n",
    "    url\n",
    "        # make the GET request\n",
    "    results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
    "        \n",
    "        # return only relevant information for each nearby venue\n",
    "        # return only relevant information for each nearby venue\n",
    "    venues_list.append([(\n",
    "        name, \n",
    "        lat, \n",
    "        lng, \n",
    "        v['venue']['name'], \n",
    "        v['venue']['location']['lat'], \n",
    "        v['venue']['location']['lng'],  \n",
    "        v['venue']['categories'][0]['name']) for v in results])\n",
    "\n",
    "nearby_venues = pd.DataFrame([item for venues_list in venues_list for item in venues_list])\n",
    "nearby_venues.columns = ['Neighborhood', \n",
    "                'Neighborhood Latitude', \n",
    "                'Neighborhood Longitude', \n",
    "                'Venue', \n",
    "                'Venue Latitude', \n",
    "                'Venue Longitude', \n",
    "                'Venue Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearby_venues.to_csv('venuesdatas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearby_venues.groupby(['Neighborhood','Venue Category']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not run foresquare because of limit, use backup\n",
    "nearby_venues=pd.read_csv('venuesdata.csv')\n",
    "nearby_venues['Neighborhood']=nearby_venues['Neighborhood'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "onehot = pd.get_dummies(nearby_venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
    "# add neighborhood column back to dataframe\n",
    "onehot1 = nearby_venues['Neighborhood'] \n",
    "outdoor = onehot['Waterfront']+onehot['Waterfall']+onehot['Bay'] + onehot['Beach']+onehot['Nature Preserve']+onehot['Outdoors & Recreation']+onehot['Park']+onehot['Trail']+onehot['Bike Trail']\n",
    "Sports = onehot['Recreation Center']+onehot['Track']+onehot['Sports Club']+onehot['Track']+onehot['Yoga Studio']+onehot['Climbing Gym']+onehot['Cycle Studio']+onehot['Gym / Fitness Center']+onehot['Pilates Studio']\n",
    "Coffe = onehot['Café']+onehot['Coffee Shop']+onehot['Tea Room']\n",
    "#Lib = onehot['College Library']+onehot['Library']\n",
    "#onehot.drop(labels=['Neighborhood'],axis=1,inplace=True)\n",
    "#onehot2=onehot2.append({'zip_code':onehot1,'beach':Beach,'sports':Sports,'coffee':Coffe,'librart':Lib},ignore_index=True)\n",
    "onehot.insert(0,'Neighborhood',onehot1)\n",
    "onehot.insert(1,'outdoor',outdoor)\n",
    "onehot.insert(2,'sports',Sports)\n",
    "onehot.insert(3,'Coffee/tea',Coffe)\n",
    "#onehot.insert(4,'lib',Lib)\n",
    "onehot2 = onehot[['Neighborhood','Coffee/tea','outdoor','sports']]\n",
    "onehot2.head\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_grouped = onehot2.groupby('Neighborhood').mean().reset_index()\n",
    "la_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_most_common_venues(row, num_top_venues):\n",
    "    row_categories = row.iloc[1:]\n",
    "    row_categories_sorted = row_categories.sort_values(ascending=False)\n",
    "    \n",
    "    return row_categories_sorted.index.values[0:num_top_venues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_venues = 3\n",
    "\n",
    "indicators = ['st', 'nd', 'rd']\n",
    "\n",
    "# create columns according to number of top venues\n",
    "columns = ['Neighborhood']\n",
    "for ind in np.arange(num_top_venues):\n",
    "    try:\n",
    "        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))\n",
    "    except:\n",
    "        columns.append('{}th Most Common Venue'.format(ind+1))\n",
    "\n",
    "# create a new dataframe\n",
    "neighborhoods_venues_sorted = pd.DataFrame(columns=columns)\n",
    "neighborhoods_venues_sorted['Neighborhood'] = la_grouped['Neighborhood']\n",
    "\n",
    "for ind in np.arange(la_grouped.shape[0]):\n",
    "    neighborhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(la_grouped.iloc[ind, :], num_top_venues)\n",
    "\n",
    "neighborhoods_venues_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kclusters = 5\n",
    "\n",
    "la_grouped_clustering = la_grouped.drop('Neighborhood', 1)\n",
    "\n",
    "# run k-means clustering\n",
    "kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(la_grouped_clustering)\n",
    "\n",
    "# check cluster labels generated for each row in the dataframe\n",
    "kmeans.labels_[0:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhoods_venues_sorted.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add clustering labels\n",
    "\n",
    "neighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)\n",
    "neighborhoods_venues_sorted.rename({'Neighborhood':'zip code'},axis ='columns',inplace =True)\n",
    "la_merged = la_df\n",
    "\n",
    "## merge toronto_grouped with toronto_data to add latitude/longitude for each neighborhood\n",
    "la_merged = la_merged.join(neighborhoods_venues_sorted.set_index('zip code'), on='zip code')\n",
    "\n",
    "#neighborhoods_venues_sorted.head() # check the last columns!\n",
    "la_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "la_merged['Cluster Labels'].fillna(0, inplace=True)\n",
    "la_merged['Cluster Labels']=la_merged['Cluster Labels'].astype(int)\n",
    "la_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_merged.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamap=folium.Map(location = [latitude,longitude],\n",
    "default_zoom_start=12)\n",
    "Zipc='ZIPCodes.geojson'\n",
    "folium.Choropleth(\n",
    "    geo_data=Zipc,\n",
    "    data=la_merged,\n",
    "    columns=['zip code','condo price'],\n",
    "    key_on='feature.properties.zipcode',\n",
    "    fill_opacity=0.3,\n",
    "    line_opacity=0.2,\n",
    "    line_weight=2,\n",
    "    legend_name='condo price',\n",
    "    fill_color='RdYlGn'\n",
    ").add_to(lamap)\n",
    "folium.LayerControl().add_to(lamap)\n",
    "# set color scheme for the clusters\n",
    "x = np.arange(kclusters)\n",
    "ys = [i + x + (i*x)**2 for i in range(kclusters)]\n",
    "colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
    "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
    "\n",
    "# add markers to the map\n",
    "markers_colors = []\n",
    "for loc, poi, cluster ,hincome ,liab,hprice in zip(la_merged['geo_zipcode'], la_merged['zip code'], la_merged['Cluster Labels'],la_merged['income'],la_merged['liability'],la_merged['condo price']):\n",
    "    geo = pd.DataFrame.from_records(data=loc)\n",
    "    lon = (geo.coordinates[0])\n",
    "    lat= (geo.coordinates[1])\n",
    "    #label = folium.Popup(str(poi) + ' Cluster ' + str(cluster)+' Unemployment ' + unemp +'Income '+income +'price'+price , parse_html=True)\n",
    "    label =  str(poi) + ' Cluster ' + str(cluster) + 'liability:' + '{}'.format(liab) +'Income :'+'{}'.format(hincome) +'condo price: '+'{}'.format( hprice)\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    if cluster != 0 and hprice < 600000:\n",
    "        folium.CircleMarker(\n",
    "            [lat, lon],\n",
    "            radius=5,\n",
    "            popup=label,\n",
    "            color=rainbow[cluster-1],\n",
    "            fill=True,\n",
    "            fill_color=rainbow[cluster-1],\n",
    "            fill_opacity=0.7).add_to(lamap)\n",
    "lamap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to use 3 zip code 90744, ,90008,90068 to create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_merged.loc[la_merged['Cluster Labels'] == 4, la_merged.columns[[0] +list(range(1, la_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_grp=la_merged[la_merged['Cluster Labels']== 4]\n",
    "la_grp.groupby('zip code').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DistributionPlot(RedFunction, BlueFunction, RedName, BlueName, Title):\n",
    "    width = 12\n",
    "    height = 10\n",
    "    plt.figure(figsize=(width, height))\n",
    "\n",
    "    ax1 = sns.distplot(RedFunction, hist=False, color=\"r\", label=RedName)\n",
    "    ax2 = sns.distplot(BlueFunction, hist=False, color=\"b\", label=BlueName, ax=ax1)\n",
    "\n",
    "    plt.title(Title)\n",
    "    plt.xlabel('foreclosure')\n",
    "    plt.ylabel('Proportion ')\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create mulitiple regression with train & test\n",
    "mydataz=mydatas\n",
    "\n",
    "lr1 = LinearRegression()\n",
    "label=mydataz['foreclosure']\n",
    "train1 = mydataz.drop(['foreclosure','lat','lon','yearmonth','zip code'],axis=1)\n",
    "    \n",
    "#Creating train / test set\n",
    "x_train, x_test, y_train, y_test = train_test_split(train1, label, test_size=0.20, random_state=4)\n",
    "\n",
    "lr1.fit(x_train, y_train)\n",
    "print(\"number of test samples :\", x_test.shape[0])\n",
    "print(\"number of training samples:\",x_train.shape[0])\n",
    "yhat_train = lr1.predict(x_train)\n",
    "yhat_test = lr1.predict(x_test)\n",
    "\n",
    "Title = 'Distribution  Plot of  Predicted Value Using Training Data vs Training Data Distribution'\n",
    "DistributionPlot(y_train, yhat_train, \"Actual Values (Train)\", \"Predicted Values (Train)\", Title)\n",
    "Title='Distribution  Plot of  Predicted Value Using Test Data vs Data Distribution of Test Data'\n",
    "DistributionPlot(y_test,yhat_test,\"Actual Values (Test)\",\"Predicted Values (Test)\",Title)\n",
    "    # visualizing residuals\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "residuals = (y_test- yhat_test)\n",
    "sns.distplot(residuals)\n",
    "#    print (mydataz.head())\n",
    "#    house1 =train1.loc[(train1['condprice'] > 308300)]\n",
    "print('The R-square Linear is: ', lr1.score(train1, mydataz['foreclosure']))\n",
    "#compare actual output values with predicted values\n",
    "# evaluate the performance of the algorithm (MAE - MSE - RMSE)\n",
    "\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, yhat_test))  \n",
    "print('MSE:', metrics.mean_squared_error(y_test, yhat_test))  \n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, yhat_test)))\n",
    "print('VarScore:',metrics.explained_variance_score(y_test,yhat_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 6,\n",
    "          'min_samples_split': 2,\n",
    "          'learning_rate': 0.09,\n",
    "          'loss': 'ls'}\n",
    "   # params = {'n_estimators': 500,\n",
    "  #        'max_depth': 4,\n",
    "  #        'min_samples_split': 2,\n",
    "  #        'learning_rate': 0.1,\n",
    "  #        'loss': 'quantile',\n",
    "  #        'alpha': .78}\n",
    "label=mydataz['foreclosure']\n",
    "train1 = mydataz.drop(['foreclosure','lat','lon','yearmonth','zip code'],axis=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(train1, label, test_size=0.30, random_state=6)\n",
    "clf = ensemble.GradientBoostingRegressor(**params)\n",
    "clf.fit(x_train,y_train)    \n",
    "clf.score(x_train,y_train)\n",
    "yhat_test2=clf.predict(x_test)\n",
    "mse = mean_squared_error(y_test, clf.predict(x_test))\n",
    "print('The GRADIENT BOOST R-square is: ', clf.score(x_train,y_train))\n",
    "\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, yhat_test2))  \n",
    "print('MSE:', metrics.mean_squared_error(y_test, yhat_test2))  \n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, yhat_test2)))\n",
    "print('VarScore:',metrics.explained_variance_score(y_test,yhat_test2))\n",
    "\n",
    "test_score = np.zeros((params['n_estimators'],), dtype=np.float64)\n",
    "for i, y_pred in enumerate(clf.staged_predict(x_test)):\n",
    "    test_score[i] = clf.loss_(y_test, y_pred)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.title('Deviance')\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, clf.train_score_, 'b-',\n",
    "         label='Training Set Deviance')\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',\n",
    "         label='Test Set Deviance')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Boosting Iterations')\n",
    "plt.ylabel('Deviance')\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "    #check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOW I WILL EVALUATE ZIP CODE 90008/90023/90018\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check result\n",
    "\n",
    "\n",
    "house1 =train1.loc[(mydatas['zip code']== 90023 ) & ( mydatas['unemployment_rate'] > 0.0) & ( mydatas['unemployment_rate'] < 4.5)]\n",
    "print('Liner :',lr1.predict(house1).mean())\n",
    "print('Grad :',clf.predict(house1).mean())\n",
    "house2 =mydatas.loc[(mydatas['zip code']== 90023 ) & ( mydatas['unemployment_rate'] > 0.0) & ( mydatas['unemployment_rate'] < 4.5)]\n",
    "#print (house2)\n",
    "print(house2.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check\n",
    "\n",
    "house1 =train1.loc[(mydatas['zip code']== 90018 ) & ( mydatas['unemployment_rate'] > 5.0) & ( mydatas['unemployment_rate'] < 5.5)]\n",
    "print('Liner :',lr1.predict(house1).mean())\n",
    "print(clf.predict(house1).mean())\n",
    "house2 =mydatas.loc[(mydatas['zip code']== 90018 ) & ( mydatas['unemployment_rate'] > 5.0) & ( mydatas['unemployment_rate'] < 5.5)]\n",
    "#print (house2)\n",
    "print(house2.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house1 =train1.loc[(mydatas['zip code']== 90008 ) & ( mydatas['unemployment_rate'] > 0.0) & ( mydatas['unemployment_rate'] < 4.5)]\n",
    "print('Liner :',lr1.predict(house1).mean())\n",
    "print('Grad :',clf.predict(house1).mean())\n",
    "house2 =mydatas.loc[(mydatas['zip code']== 90008 ) & ( mydatas['unemployment_rate'] > 0.0) & ( mydatas['unemployment_rate'] < 4.5)]\n",
    "#print (house2)\n",
    "print(house2.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
